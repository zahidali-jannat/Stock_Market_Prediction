{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d29c2b7f-96de-46e5-ac1a-3444f0b1f2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\za220\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\za220\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\za220\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\za220\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\za220\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\za220\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\za220\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\za220\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\za220\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\za220\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\za220\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\za220\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\za220\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\za220\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\za220\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\za220\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\za220\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\za220\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\za220\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\za220\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense , Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ac4009f-a77e-469f-bc8f-fe06a7c77bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date=\"2015-9-16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3d06aaf-15a7-4995-a907-9237b6900827",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date=datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "508d294f-399a-44c0-9402-7dbab6965e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol=['AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85cd81e9-1829-48c5-bea7-2618ab09cd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\za220\\AppData\\Local\\Temp\\ipykernel_23324\\2694260390.py:1: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  AAPL=yf.download(symbol , start_date , end_date, multi_level_index=False).Close\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2015-09-16     26.160305\n",
       "2015-09-17     25.600735\n",
       "2015-09-18     25.495115\n",
       "2015-09-21     25.890631\n",
       "2015-09-22     25.483881\n",
       "                 ...    \n",
       "2025-09-15    236.699997\n",
       "2025-09-16    238.149994\n",
       "2025-09-17    238.990005\n",
       "2025-09-18    237.880005\n",
       "2025-09-19    245.500000\n",
       "Name: Close, Length: 2518, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AAPL=yf.download(symbol , start_date , end_date, multi_level_index=False).Close\n",
    "AAPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d888a18-1d62-4ca1-8862-a7ea49c6a14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02331253],\n",
       "       [0.02095625],\n",
       "       [0.0205115 ],\n",
       "       ...,\n",
       "       [0.91951428],\n",
       "       [0.91484019],\n",
       "       [0.94692712]], shape=(2518, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step-2:\n",
    "#Intilazing MinMaxScaler to normalize the data between 0 and 1\n",
    "scaler=MinMaxScaler(feature_range=(0 , 1))\n",
    "\n",
    "data=AAPL.values.reshape(-1 , 1)\n",
    "# Scaled the data for training \n",
    "scaled_data=scaler.fit_transform(data)\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "421b15ab-f148-4205-9297-472dc67c2f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Step-3 \n",
    "# Defining training data length as 80% of the total data\n",
    "training_data_len=int(len(scaled_data) * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fee5bf5-93f5-4c6a-bf49-d64dd514cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training set \n",
    "training_data=scaled_data[: training_data_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2689bbb-4a65-42a5-93fc-60e96e26204d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02331253],\n",
       "       [0.02095625],\n",
       "       [0.0205115 ],\n",
       "       ...,\n",
       "       [0.63961741],\n",
       "       [0.64599754],\n",
       "       [0.64295336]], shape=(2014, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38a1ab42-5f83-4bbc-9d4a-e688962a4515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty list for features (x_train and y_train) \n",
    "x_train=[]\n",
    "y_train=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b9dfd44-84c5-40cb-8cde-1c2b1f567d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24afa36c-5b38-429c-a7cc-8907b3ff706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(60 , len(training_data)):\n",
    "    x_train.append(training_data[i-60:i , 0])\n",
    "    y_train.append(training_data[i , 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e6aae1e-4d67-4c8b-ba85-a1932b6776f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list to numpy _ array t0 model training \n",
    "x_train=np.array(x_train)\n",
    "y_train=np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa4f7307-21d8-40aa-aa78-b77033cdf642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert it into 3d sample if we want to train lstm model convert it to the 3d sample\n",
    "x_train=np.reshape(x_train ,(x_train.shape[0] , x_train.shape[1] , 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea205674-74db-4953-879b-b8ef311f88df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1954, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape) # u change the dimension of vector from 2d to 3d for modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f249fabf-e08a-4d4a-a669-9818486b0024",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "786d470d-e31d-4f23-9490-c8415a731641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\za220\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# First lstm layer with 50 units and return_sequence\n",
    "model.add(LSTM(units=200 , return_sequences=True, input_shape=(x_train.shape[1] , 1)))\n",
    "model.add(Dropout(0.2)) # Dropout layer to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6094ba3a-0055-4452-a857-da7fb3b00eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second layer of lstm layer \n",
    "model.add(LSTM(units=50 , return_sequences=False))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa817d17-6e4d-4c31-857a-5a85c429c2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dense layer with 25 units \n",
    "model.add(Dense(units=25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f81ed6b8-3f64-4c33-8446-88090c0f4869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output layer with 1 units (Predicted price)\n",
    "model.add(Dense(units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd104b63-db9f-43f2-89ea-5901d2fd0cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model using adam optimizer and mean squared error as loss function\n",
    "model.compile(optimizer='adam' , loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51585eec-a518-4586-be74-7000f45aeced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model after stopping\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59f26c59-e667-4157-a018-9551e5373b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown variable: <Variable path=sequential/lstm/lstm_cell/kernel, shape=(1, 800), dtype=float32, value=[[-0.07521152  0.08307968 -0.04478868 -0.01839498  0.06848621  0.07921456\n   0.07147989  0.05635913  0.04476336 -0.00703485  0.02158105  0.05066474\n  -0.05996299 -0.06775461 -0.03211879 -0.01025771  0.07122885  0.08494237\n  -0.05582442 -0.04708673 -0.01017158  0.05892153 -0.04850046  0.08274643\n   0.03028873 -0.02138199  0.04585086 -0.00269926  0.00686636 -0.03545999\n   0.01976364 -0.00067011  0.02684211 -0.02295706  0.01525664  0.07034844\n  -0.03477845  0.05419429 -0.03788225 -0.01710232 -0.03790756  0.07542583\n  -0.04429718 -0.02665419  0.07045162 -0.06779786  0.02623165 -0.05037011\n  -0.07589525  0.0706827  -0.05672599 -0.02050743  0.03449781 -0.05220219\n  -0.0080957  -0.03383814 -0.01252462  0.01019112  0.03120124  0.00091154\n   0.03904487  0.06338827  0.00720126  0.01122434  0.00412322 -0.04182903\n  -0.00982308 -0.00112174 -0.08519299 -0.04202034 -0.01560345  0.03338917\n  -0.06437705 -0.00594261 -0.06382845  0.06805955  0.01988591 -0.06629004\n   0.08512039  0.00312792  0.04636435 -0.0157566   0.0793763   0.04218012\n  -0.02969573 -0.03577593 -0.03277033 -0.04053043  0.06663817  0.08028997\n  -0.0522031  -0.00176149 -0.03551137 -0.04362946  0.02817369 -0.00396303\n  -0.06472956 -0.00331178  0.07233658 -0.04160932  0.00714894 -0.00369148\n   0.02368918  0.07238901  0.00413772  0.00245164  0.02867384 -0.08115123\n   0.0540186  -0.07924855  0.02820832  0.08076239 -0.01206548 -0.01335777\n   0.06061597  0.00779371  0.01820411 -0.03236996  0.03746811  0.00246786\n   0.02973417  0.05831078 -0.07021844 -0.02786716  0.03983831  0.06484988\n   0.06555156 -0.05284548  0.0321704   0.03125958  0.04912469 -0.05532035\n   0.03023748 -0.07456194  0.05274279 -0.0746459  -0.06204819  0.06330658\n  -0.01388399 -0.04184895  0.07948695  0.04210931  0.03489093  0.08467942\n   0.05485295 -0.07568806  0.01767396  0.02676915 -0.04410484  0.01798739\n   0.01887555  0.05247733 -0.05692933  0.04246931 -0.04412067  0.0282229\n  -0.08129618  0.07770798 -0.0801254   0.000296   -0.0305462   0.08436088\n  -0.00968352 -0.04370244  0.02728019  0.08283299  0.03199574  0.01214748\n   0.03038122 -0.07375105  0.05084766 -0.01444522  0.00024685 -0.00271539\n  -0.05200319  0.08220018  0.02483658  0.07702799 -0.01348794 -0.02050528\n   0.04834388 -0.01735185 -0.07772793 -0.03951266  0.06269532 -0.02168491\n   0.03753298  0.039878   -0.08196166  0.07835592  0.05480585  0.01242\n   0.05573143 -0.00591853  0.0398937   0.04251318 -0.05695481 -0.07089161\n  -0.0134914   0.04627132 -0.02639851 -0.08646417 -0.01056269 -0.02317874\n  -0.01378874 -0.01142079  0.0348406  -0.07593062 -0.05939002  0.00797264\n   0.06592916 -0.01586563  0.05757149  0.06515069 -0.07339516  0.03608644\n  -0.01281933  0.06806315 -0.07881559  0.00646897 -0.04738199  0.0550209\n  -0.01559494  0.03292718 -0.00505195  0.01816158 -0.07251317  0.00732876\n  -0.03631014  0.01566336 -0.07502558  0.0628541   0.02516531  0.02230424\n  -0.01389747  0.03643987 -0.02730627 -0.01612873  0.06343441  0.01262756\n  -0.06245581  0.06107432 -0.04870638  0.0293695   0.05971448  0.07022865\n   0.05615941  0.07133998 -0.0163736   0.02774011  0.06859608  0.04690038\n  -0.06943408  0.08026792 -0.01632345 -0.07245828 -0.06308559 -0.07714301\n   0.00878245 -0.03448758  0.02403545 -0.03706338  0.08028032  0.06778915\n  -0.0406849   0.06391676  0.07358456 -0.05450524 -0.03853368 -0.07999567\n  -0.02684096  0.01194225  0.06039572 -0.08158869  0.0317824   0.03627622\n   0.04810914  0.00143952  0.01531244  0.04574575 -0.00657646 -0.04167089\n   0.02171687 -0.06870622 -0.01641046  0.01078154 -0.02578479 -0.00209647\n   0.02760473  0.01634702 -0.03755737  0.02874051 -0.02938227  0.0410437\n  -0.026172   -0.05139413  0.02826739 -0.05841016  0.05588371 -0.01574094\n  -0.07280359 -0.03643746  0.00031571  0.08012928 -0.00327622 -0.00752036\n   0.07358408  0.0553752  -0.04810937 -0.05084525 -0.03720254  0.06974207\n   0.04461534 -0.08541885  0.07766621  0.03952287  0.03298201  0.03964068\n   0.0162109  -0.06220165 -0.07351716  0.0152585   0.08597723  0.02032212\n  -0.02311733  0.06621715 -0.00673876 -0.0624452  -0.04734186  0.06651801\n  -0.06445594  0.04387635  0.04439835 -0.03035598 -0.05708421  0.06924801\n  -0.04632824  0.04689899 -0.02654538  0.01455861  0.05255327 -0.08471599\n  -0.06065442  0.02849402  0.05429713 -0.02849725  0.029679   -0.02483637\n  -0.01540641 -0.01431792 -0.03044467  0.04828505  0.02962428  0.06575976\n   0.01083463  0.07856025  0.01888689 -0.02532651  0.01886962 -0.00755457\n   0.07888484  0.01223662  0.0552498  -0.01876344 -0.0143848   0.0068104\n   0.03612082  0.00446089  0.02897128  0.03928019  0.00141385 -0.07016456\n   0.01599862 -0.05415176  0.05722603  0.07028736 -0.03882257 -0.02646872\n   0.02350871  0.0107927  -0.0234936  -0.075337   -0.06650786  0.05289578\n   0.0816931  -0.07826988 -0.04455276  0.06870046 -0.02900785 -0.0670854\n   0.00269948  0.02658335  0.05258881  0.04507901  0.04135637  0.04544111\n  -0.08081291 -0.07184351  0.0366942   0.02747189 -0.00961838 -0.0791143\n   0.05219573  0.05402753 -0.05725899  0.01089125 -0.01507712 -0.00959118\n  -0.06096951  0.06593935 -0.07969479 -0.07197283  0.05769113  0.03127935\n  -0.06857686 -0.07286473  0.05245022  0.0446538   0.01623642  0.03603007\n   0.0480096  -0.00475576  0.05106005 -0.00541279 -0.01112983  0.054298\n   0.08197322  0.04849313 -0.03698775 -0.04416718  0.04773039  0.07953052\n  -0.05613598 -0.07861438 -0.06413412  0.08440392  0.05813365  0.05533284\n  -0.07253888  0.02689786 -0.07914419  0.05719686  0.04743284 -0.06127756\n   0.02773353 -0.04243248  0.0772091  -0.02932641  0.06487167 -0.07475611\n   0.08275965  0.05637701  0.06230615 -0.01002292  0.0340085   0.03992422\n   0.01245914 -0.07473977 -0.01683903 -0.01123197  0.00170457  0.07520135\n   0.03403471  0.07650758 -0.03655188  0.03202513 -0.00085622 -0.07562576\n   0.01628186 -0.03356265 -0.03996582  0.02417141  0.06276129  0.01860529\n   0.08478603  0.02424648 -0.08218834  0.0167459  -0.01320647  0.03501723\n  -0.05056082 -0.0582606   0.05661578  0.07320128  0.02335634 -0.05765978\n   0.06997335 -0.0734407   0.08593634  0.02230168 -0.02929218  0.05707741\n   0.02415367 -0.03134387 -0.00817896 -0.0100476   0.02502305 -0.04493129\n  -0.02172112  0.01469184 -0.04905432 -0.07138874 -0.06174732 -0.04907493\n   0.05396001  0.03426614 -0.00413543  0.06110506 -0.06323294  0.0453864\n  -0.01513521  0.02626595 -0.06941979  0.03274757  0.05413991  0.01683313\n  -0.03796986  0.03397435  0.00539133  0.03157183  0.0393807   0.04967515\n   0.07037549  0.02884762 -0.0522245  -0.07297989 -0.05419433 -0.0226581\n   0.04905744  0.08336774 -0.00035851 -0.03198016 -0.02671246 -0.07207609\n  -0.06541488  0.0535212   0.03277056  0.06843542 -0.022576   -0.04912218\n  -0.01891384  0.05156612  0.06080194  0.06311004  0.04281902  0.06135726\n   0.05568053  0.0603231   0.07797129  0.03250899 -0.06508666  0.02515408\n  -0.02243593  0.04370081  0.03553234 -0.055563   -0.06622595 -0.03282733\n   0.04654175  0.04860866 -0.04994574  0.04361413 -0.0227192   0.03938398\n   0.08575772 -0.02705485  0.07121265 -0.06857629 -0.08593941  0.06704248\n   0.07254499  0.06815147  0.07491831 -0.02442279 -0.07051404 -0.03790449\n  -0.08274882 -0.08095277  0.00414261 -0.00916708 -0.05431032  0.01822221\n  -0.05075194 -0.01578283  0.0625371  -0.02921519  0.01670357 -0.01410582\n   0.02222273 -0.01051329 -0.06633143 -0.02244738  0.0060515  -0.0569104\n   0.01030273  0.04352631  0.00051657 -0.0079994  -0.06664172  0.01655805\n   0.05191745 -0.07128356 -0.0081249  -0.01938931  0.02126177  0.01786762\n   0.06723312  0.03842442  0.06143573 -0.02256549  0.06028046 -0.00701375\n   0.08190115  0.04008307  0.06443733  0.08375847 -0.08261863 -0.01495767\n   0.07957499 -0.02405656  0.03835744 -0.03608052  0.03794068  0.00836537\n  -0.04625406 -0.06917785  0.05111459  0.01202553 -0.01437808  0.01659596\n  -0.00579119 -0.02020374 -0.07920517 -0.00279351  0.0168172   0.01295912\n   0.02716956  0.03722352 -0.02504794  0.04422016 -0.07527918 -0.06066187\n  -0.06886833  0.00814817 -0.04563889 -0.040744   -0.00713127 -0.0634438\n  -0.06836092 -0.01202674  0.0683111   0.06033222  0.03906511 -0.00464813\n   0.03565796  0.07989933  0.0124362   0.02142074 -0.003814   -0.06238495\n   0.00383551 -0.05210917  0.03069375  0.00741209 -0.08012337  0.04998629\n  -0.04368825 -0.00773548 -0.03560227 -0.03109943 -0.06532802  0.04108801\n  -0.05180087  0.07556757  0.05900405  0.03413408  0.00875624 -0.05047027\n  -0.01061656  0.04134852 -0.04392231  0.07080193  0.07573317  0.01580175\n  -0.00557273 -0.03875899 -0.07051746  0.06639209 -0.02216545  0.0154938\n   0.06197673  0.03003807 -0.04954452  0.07954197 -0.04685878  0.07780203\n   0.00435146  0.03665254 -0.00702814 -0.01060334  0.03925447 -0.06357545\n  -0.0607528   0.06157699 -0.0277643  -0.02288438  0.00608981  0.06364739\n   0.0828331  -0.01229902  0.07371992 -0.07422087  0.0272221   0.02859844\n   0.07318586 -0.04348293 -0.00904847  0.0469116   0.06055516 -0.0580461\n   0.03459273 -0.00941919  0.04359533  0.01488592  0.000875   -0.03539916\n  -0.05774564  0.05094273  0.02831514 -0.05193223 -0.0140786  -0.02055307\n  -0.01810481  0.03826397 -0.04211534 -0.06601334 -0.03671021  0.08404328\n  -0.02673762  0.04230891  0.0339163   0.04615165 -0.05969663 -0.07818994\n   0.05480586  0.06471373  0.02876624  0.05082643 -0.04114233 -0.0609795\n   0.03085863 -0.04383397  0.07704133 -0.00897513 -0.06773765  0.00497382\n   0.03886701 -0.07399844 -0.02443094  0.01293135 -0.03384293  0.0266019\n  -0.01736429 -0.02637267 -0.05007906 -0.07167676 -0.02180729  0.02655413\n   0.00772006 -0.00496309 -0.08508546 -0.00452648  0.05783367  0.08176516\n   0.07779254 -0.01509711 -0.01444695 -0.00010474  0.01456416  0.03266295\n   0.06663229 -0.00207967 -0.02004958  0.00133134 -0.03610386 -0.06546514\n  -0.00794525 -0.03898476 -0.019586    0.06726508  0.0421509  -0.06450261\n  -0.06774899 -0.05505718  0.05873115  0.00629409 -0.0057547  -0.06759504\n   0.02265713 -0.0354355 ]]>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x_train , y_train , batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m , epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:409\u001b[0m, in \u001b[0;36mBaseOptimizer._check_variables_are_known\u001b[1;34m(self, variables)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m variables:\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(v) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_variables_indices:\n\u001b[1;32m--> 409\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This optimizer can only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe called for the variables it was originally built with. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen working with a new set of variables, you should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    413\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecreate a new optimizer instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    414\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown variable: <Variable path=sequential/lstm/lstm_cell/kernel, shape=(1, 800), dtype=float32, value=[[-0.07521152  0.08307968 -0.04478868 -0.01839498  0.06848621  0.07921456\n   0.07147989  0.05635913  0.04476336 -0.00703485  0.02158105  0.05066474\n  -0.05996299 -0.06775461 -0.03211879 -0.01025771  0.07122885  0.08494237\n  -0.05582442 -0.04708673 -0.01017158  0.05892153 -0.04850046  0.08274643\n   0.03028873 -0.02138199  0.04585086 -0.00269926  0.00686636 -0.03545999\n   0.01976364 -0.00067011  0.02684211 -0.02295706  0.01525664  0.07034844\n  -0.03477845  0.05419429 -0.03788225 -0.01710232 -0.03790756  0.07542583\n  -0.04429718 -0.02665419  0.07045162 -0.06779786  0.02623165 -0.05037011\n  -0.07589525  0.0706827  -0.05672599 -0.02050743  0.03449781 -0.05220219\n  -0.0080957  -0.03383814 -0.01252462  0.01019112  0.03120124  0.00091154\n   0.03904487  0.06338827  0.00720126  0.01122434  0.00412322 -0.04182903\n  -0.00982308 -0.00112174 -0.08519299 -0.04202034 -0.01560345  0.03338917\n  -0.06437705 -0.00594261 -0.06382845  0.06805955  0.01988591 -0.06629004\n   0.08512039  0.00312792  0.04636435 -0.0157566   0.0793763   0.04218012\n  -0.02969573 -0.03577593 -0.03277033 -0.04053043  0.06663817  0.08028997\n  -0.0522031  -0.00176149 -0.03551137 -0.04362946  0.02817369 -0.00396303\n  -0.06472956 -0.00331178  0.07233658 -0.04160932  0.00714894 -0.00369148\n   0.02368918  0.07238901  0.00413772  0.00245164  0.02867384 -0.08115123\n   0.0540186  -0.07924855  0.02820832  0.08076239 -0.01206548 -0.01335777\n   0.06061597  0.00779371  0.01820411 -0.03236996  0.03746811  0.00246786\n   0.02973417  0.05831078 -0.07021844 -0.02786716  0.03983831  0.06484988\n   0.06555156 -0.05284548  0.0321704   0.03125958  0.04912469 -0.05532035\n   0.03023748 -0.07456194  0.05274279 -0.0746459  -0.06204819  0.06330658\n  -0.01388399 -0.04184895  0.07948695  0.04210931  0.03489093  0.08467942\n   0.05485295 -0.07568806  0.01767396  0.02676915 -0.04410484  0.01798739\n   0.01887555  0.05247733 -0.05692933  0.04246931 -0.04412067  0.0282229\n  -0.08129618  0.07770798 -0.0801254   0.000296   -0.0305462   0.08436088\n  -0.00968352 -0.04370244  0.02728019  0.08283299  0.03199574  0.01214748\n   0.03038122 -0.07375105  0.05084766 -0.01444522  0.00024685 -0.00271539\n  -0.05200319  0.08220018  0.02483658  0.07702799 -0.01348794 -0.02050528\n   0.04834388 -0.01735185 -0.07772793 -0.03951266  0.06269532 -0.02168491\n   0.03753298  0.039878   -0.08196166  0.07835592  0.05480585  0.01242\n   0.05573143 -0.00591853  0.0398937   0.04251318 -0.05695481 -0.07089161\n  -0.0134914   0.04627132 -0.02639851 -0.08646417 -0.01056269 -0.02317874\n  -0.01378874 -0.01142079  0.0348406  -0.07593062 -0.05939002  0.00797264\n   0.06592916 -0.01586563  0.05757149  0.06515069 -0.07339516  0.03608644\n  -0.01281933  0.06806315 -0.07881559  0.00646897 -0.04738199  0.0550209\n  -0.01559494  0.03292718 -0.00505195  0.01816158 -0.07251317  0.00732876\n  -0.03631014  0.01566336 -0.07502558  0.0628541   0.02516531  0.02230424\n  -0.01389747  0.03643987 -0.02730627 -0.01612873  0.06343441  0.01262756\n  -0.06245581  0.06107432 -0.04870638  0.0293695   0.05971448  0.07022865\n   0.05615941  0.07133998 -0.0163736   0.02774011  0.06859608  0.04690038\n  -0.06943408  0.08026792 -0.01632345 -0.07245828 -0.06308559 -0.07714301\n   0.00878245 -0.03448758  0.02403545 -0.03706338  0.08028032  0.06778915\n  -0.0406849   0.06391676  0.07358456 -0.05450524 -0.03853368 -0.07999567\n  -0.02684096  0.01194225  0.06039572 -0.08158869  0.0317824   0.03627622\n   0.04810914  0.00143952  0.01531244  0.04574575 -0.00657646 -0.04167089\n   0.02171687 -0.06870622 -0.01641046  0.01078154 -0.02578479 -0.00209647\n   0.02760473  0.01634702 -0.03755737  0.02874051 -0.02938227  0.0410437\n  -0.026172   -0.05139413  0.02826739 -0.05841016  0.05588371 -0.01574094\n  -0.07280359 -0.03643746  0.00031571  0.08012928 -0.00327622 -0.00752036\n   0.07358408  0.0553752  -0.04810937 -0.05084525 -0.03720254  0.06974207\n   0.04461534 -0.08541885  0.07766621  0.03952287  0.03298201  0.03964068\n   0.0162109  -0.06220165 -0.07351716  0.0152585   0.08597723  0.02032212\n  -0.02311733  0.06621715 -0.00673876 -0.0624452  -0.04734186  0.06651801\n  -0.06445594  0.04387635  0.04439835 -0.03035598 -0.05708421  0.06924801\n  -0.04632824  0.04689899 -0.02654538  0.01455861  0.05255327 -0.08471599\n  -0.06065442  0.02849402  0.05429713 -0.02849725  0.029679   -0.02483637\n  -0.01540641 -0.01431792 -0.03044467  0.04828505  0.02962428  0.06575976\n   0.01083463  0.07856025  0.01888689 -0.02532651  0.01886962 -0.00755457\n   0.07888484  0.01223662  0.0552498  -0.01876344 -0.0143848   0.0068104\n   0.03612082  0.00446089  0.02897128  0.03928019  0.00141385 -0.07016456\n   0.01599862 -0.05415176  0.05722603  0.07028736 -0.03882257 -0.02646872\n   0.02350871  0.0107927  -0.0234936  -0.075337   -0.06650786  0.05289578\n   0.0816931  -0.07826988 -0.04455276  0.06870046 -0.02900785 -0.0670854\n   0.00269948  0.02658335  0.05258881  0.04507901  0.04135637  0.04544111\n  -0.08081291 -0.07184351  0.0366942   0.02747189 -0.00961838 -0.0791143\n   0.05219573  0.05402753 -0.05725899  0.01089125 -0.01507712 -0.00959118\n  -0.06096951  0.06593935 -0.07969479 -0.07197283  0.05769113  0.03127935\n  -0.06857686 -0.07286473  0.05245022  0.0446538   0.01623642  0.03603007\n   0.0480096  -0.00475576  0.05106005 -0.00541279 -0.01112983  0.054298\n   0.08197322  0.04849313 -0.03698775 -0.04416718  0.04773039  0.07953052\n  -0.05613598 -0.07861438 -0.06413412  0.08440392  0.05813365  0.05533284\n  -0.07253888  0.02689786 -0.07914419  0.05719686  0.04743284 -0.06127756\n   0.02773353 -0.04243248  0.0772091  -0.02932641  0.06487167 -0.07475611\n   0.08275965  0.05637701  0.06230615 -0.01002292  0.0340085   0.03992422\n   0.01245914 -0.07473977 -0.01683903 -0.01123197  0.00170457  0.07520135\n   0.03403471  0.07650758 -0.03655188  0.03202513 -0.00085622 -0.07562576\n   0.01628186 -0.03356265 -0.03996582  0.02417141  0.06276129  0.01860529\n   0.08478603  0.02424648 -0.08218834  0.0167459  -0.01320647  0.03501723\n  -0.05056082 -0.0582606   0.05661578  0.07320128  0.02335634 -0.05765978\n   0.06997335 -0.0734407   0.08593634  0.02230168 -0.02929218  0.05707741\n   0.02415367 -0.03134387 -0.00817896 -0.0100476   0.02502305 -0.04493129\n  -0.02172112  0.01469184 -0.04905432 -0.07138874 -0.06174732 -0.04907493\n   0.05396001  0.03426614 -0.00413543  0.06110506 -0.06323294  0.0453864\n  -0.01513521  0.02626595 -0.06941979  0.03274757  0.05413991  0.01683313\n  -0.03796986  0.03397435  0.00539133  0.03157183  0.0393807   0.04967515\n   0.07037549  0.02884762 -0.0522245  -0.07297989 -0.05419433 -0.0226581\n   0.04905744  0.08336774 -0.00035851 -0.03198016 -0.02671246 -0.07207609\n  -0.06541488  0.0535212   0.03277056  0.06843542 -0.022576   -0.04912218\n  -0.01891384  0.05156612  0.06080194  0.06311004  0.04281902  0.06135726\n   0.05568053  0.0603231   0.07797129  0.03250899 -0.06508666  0.02515408\n  -0.02243593  0.04370081  0.03553234 -0.055563   -0.06622595 -0.03282733\n   0.04654175  0.04860866 -0.04994574  0.04361413 -0.0227192   0.03938398\n   0.08575772 -0.02705485  0.07121265 -0.06857629 -0.08593941  0.06704248\n   0.07254499  0.06815147  0.07491831 -0.02442279 -0.07051404 -0.03790449\n  -0.08274882 -0.08095277  0.00414261 -0.00916708 -0.05431032  0.01822221\n  -0.05075194 -0.01578283  0.0625371  -0.02921519  0.01670357 -0.01410582\n   0.02222273 -0.01051329 -0.06633143 -0.02244738  0.0060515  -0.0569104\n   0.01030273  0.04352631  0.00051657 -0.0079994  -0.06664172  0.01655805\n   0.05191745 -0.07128356 -0.0081249  -0.01938931  0.02126177  0.01786762\n   0.06723312  0.03842442  0.06143573 -0.02256549  0.06028046 -0.00701375\n   0.08190115  0.04008307  0.06443733  0.08375847 -0.08261863 -0.01495767\n   0.07957499 -0.02405656  0.03835744 -0.03608052  0.03794068  0.00836537\n  -0.04625406 -0.06917785  0.05111459  0.01202553 -0.01437808  0.01659596\n  -0.00579119 -0.02020374 -0.07920517 -0.00279351  0.0168172   0.01295912\n   0.02716956  0.03722352 -0.02504794  0.04422016 -0.07527918 -0.06066187\n  -0.06886833  0.00814817 -0.04563889 -0.040744   -0.00713127 -0.0634438\n  -0.06836092 -0.01202674  0.0683111   0.06033222  0.03906511 -0.00464813\n   0.03565796  0.07989933  0.0124362   0.02142074 -0.003814   -0.06238495\n   0.00383551 -0.05210917  0.03069375  0.00741209 -0.08012337  0.04998629\n  -0.04368825 -0.00773548 -0.03560227 -0.03109943 -0.06532802  0.04108801\n  -0.05180087  0.07556757  0.05900405  0.03413408  0.00875624 -0.05047027\n  -0.01061656  0.04134852 -0.04392231  0.07080193  0.07573317  0.01580175\n  -0.00557273 -0.03875899 -0.07051746  0.06639209 -0.02216545  0.0154938\n   0.06197673  0.03003807 -0.04954452  0.07954197 -0.04685878  0.07780203\n   0.00435146  0.03665254 -0.00702814 -0.01060334  0.03925447 -0.06357545\n  -0.0607528   0.06157699 -0.0277643  -0.02288438  0.00608981  0.06364739\n   0.0828331  -0.01229902  0.07371992 -0.07422087  0.0272221   0.02859844\n   0.07318586 -0.04348293 -0.00904847  0.0469116   0.06055516 -0.0580461\n   0.03459273 -0.00941919  0.04359533  0.01488592  0.000875   -0.03539916\n  -0.05774564  0.05094273  0.02831514 -0.05193223 -0.0140786  -0.02055307\n  -0.01810481  0.03826397 -0.04211534 -0.06601334 -0.03671021  0.08404328\n  -0.02673762  0.04230891  0.0339163   0.04615165 -0.05969663 -0.07818994\n   0.05480586  0.06471373  0.02876624  0.05082643 -0.04114233 -0.0609795\n   0.03085863 -0.04383397  0.07704133 -0.00897513 -0.06773765  0.00497382\n   0.03886701 -0.07399844 -0.02443094  0.01293135 -0.03384293  0.0266019\n  -0.01736429 -0.02637267 -0.05007906 -0.07167676 -0.02180729  0.02655413\n   0.00772006 -0.00496309 -0.08508546 -0.00452648  0.05783367  0.08176516\n   0.07779254 -0.01509711 -0.01444695 -0.00010474  0.01456416  0.03266295\n   0.06663229 -0.00207967 -0.02004958  0.00133134 -0.03610386 -0.06546514\n  -0.00794525 -0.03898476 -0.019586    0.06726508  0.0421509  -0.06450261\n  -0.06774899 -0.05505718  0.05873115  0.00629409 -0.0057547  -0.06759504\n   0.02265713 -0.0354355 ]]>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance."
     ]
    }
   ],
   "source": [
    "model.fit(x_train , y_train , batch_size=10 , epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f65c0f-04d2-46b2-ad21-d7ce56206518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44993bf-2a29-4c49-b516-47c0d84df034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b8e528-3fd0-47a7-b1ff-7db5431050de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60392b7-513f-448c-a136-6b0c91d1288f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d0b2ce-3058-4947-929a-94552d323894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0287a8-3a1b-4085-83d7-239ac1c1ee3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9f72a1-3bfe-43da-8c98-23505c37a183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865eff48-5aec-4332-b4d3-ed8be34f0bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341c692a-01ce-4ba4-b7ef-c651737fcf46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4711772b-7761-4cf6-9fcf-ed2aec9813bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fc91a0-257c-4fbc-b29c-c355027070ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
